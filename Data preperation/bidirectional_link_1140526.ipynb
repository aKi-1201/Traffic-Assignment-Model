{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert folder of csv format\n",
    "#ID,LENGTH,DIR,A,B,ROADNAME,OTHERNAME,PROJECTNAM,YEAR,LEVEL_110,CLASS_110,LANES_110,ROW,S0,ALPHA,BETA,NMT_110,NTK_110,NBUS_110,COUNTY,MRTXFERPEN,ETAG\n",
    "# \"4\",0.30,\"2\",\"17252\",\"10118\",台2,登輝大道,,\"104\",\"3\",\"21\",\"3\",,\"50\",1.1507,3.7967,\"0\",\"0\",\"0\",\"2\",\"0\",0.0000\n",
    "# to dat format\n",
    "# tail.node\thead.node\tcapacity..veh.h.\tlength..miles.\tfftt.min.\tPower\tB\tspeed.limit..mph.\n",
    "# 1\t54632\t99999\t0\t2.19029\t0.9\t3.6\tNA\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRTS4S_Capacity.txt\n",
    "#CLASS,L1,L2,L3,L4,L5,L6\n",
    "#eliminate empty lines\n",
    "#save as TRTS4S_Capacity.csv\n",
    "# with open('TRTS4S_Capacity.txt', 'r') as f:\n",
    "#     lines = f.readlines()\n",
    "\n",
    "# with open('TRTS4S_Capacity.csv', 'w') as f:\n",
    "#     for line in lines:\n",
    "#         if line.strip():  # Check if the line is not empty\n",
    "#             f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_dict = {}\n",
    "#input\n",
    "# CLASS,L1,L2,L3,L4,L5,L6\n",
    "# 1, 2300, 4300, 6300, 8300, 10300, 12300\n",
    "#disct: (class,lanes)=capacity eg (39,6)=9420\n",
    "with open('TRTS4S_Capacity.csv', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines[1:]:  # Skip the header line\n",
    "        parts = line.strip().split(',')\n",
    "        class_num = int(parts[0])\n",
    "        for i in range(1, len(parts)):\n",
    "            lanes = i\n",
    "            capacity = int(parts[i])\n",
    "            cap_dict[(class_num, lanes)] = capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reversible links: {(8299, 6666): 5, (6666, 8299): 3, (6666, 6628): 4, (6628, 6666): 2, (6628, 8293): 4, (8293, 6628): 2, (8293, 15665): 4, (15665, 8293): 2, (15665, 9978): 4, (9978, 15665): 2, (9978, 8289): 4, (8289, 9978): 2, (6631, 6660): 5, (6660, 6631): 3, (6660, 6642): 5, (6642, 6660): 3, (6642, 8141): 5, (8141, 6642): 3, (8141, 14039): 5, (14039, 8141): 3, (14039, 8143): 5, (8143, 14039): 3, (8143, 9883): 5, (9883, 8143): 3, (9883, 6387): 5, (6387, 9883): 3, (6291, 6696): 3, (6696, 6291): 1, (6696, 6695): 3, (6695, 6696): 1, (6695, 6629): 3, (6629, 6695): 1, (6629, 8291): 3, (8291, 6629): 1, (8345, 6682): 3, (6682, 8345): 1, (6682, 9154): 3, (9154, 6682): 1, (9154, 15473): 3, (15473, 9154): 1, (15473, 6375): 3, (6375, 15473): 1, (6375, 6683): 3, (6683, 6375): 1, (6683, 6373): 4, (6373, 6683): 2, (6373, 6409): 4, (6409, 6373): 2, (6409, 6398): 4, (6398, 6409): 2, (6753, 8090): 4, (8090, 6753): 2}\n"
     ]
    }
   ],
   "source": [
    "#調撥車道處理\n",
    "#no need to look up capacity\n",
    "#open Y110_modified_v7.shp\n",
    "gdf = gpd.read_file('Y110_modified_v7.shp')\n",
    "\n",
    "# 承德路7:00-9:00\n",
    "# [8299,6666]:(5,3) means (A,B)=(8299,6666), LANES_110=5; (A,B)=(6666,8299), LANES_110=3\n",
    "# [6666,6628,8293,15665,9978,8289]:(4,2)\n",
    "# [6631,6660,6642,8141,14039,8143,9883,6387]:(5,3)\n",
    "# 中山北路7:00-9:00\n",
    "# [6291,6696,6695,6629,8291]:(3,1)\n",
    "# 民族東路7:00-9:00\n",
    "# [8345,6682,9154,15473,6375,6683]:(3,1)\n",
    "# [6683,6373,6409,6398]:(4,2)\n",
    "# 復興北路 五常街至民權東路 07:00~09:00\n",
    "# [6753,8090]:(4,2)\n",
    "\n",
    "reversible_links_dict = {\n",
    "    (8299, 6666): (5, 3),\n",
    "    (6666, 6628, 8293, 15665, 9978, 8289): (4, 2),\n",
    "    (6631, 6660, 6642, 8141, 14039, 8143, 9883, 6387): (5, 3),\n",
    "    (6291, 6696, 6695, 6629, 8291): (3, 1),\n",
    "    (8345, 6682, 9154, 15473, 6375, 6683): (3, 1),\n",
    "    (6683, 6373, 6409, 6398): (4, 2),\n",
    "    (6753, 8090): (4, 2)\n",
    "}\n",
    "\n",
    "# 將可調撥車道轉換為字典形式\n",
    "reversible_links = {}\n",
    "for key, value in reversible_links_dict.items():\n",
    "    if isinstance(key, tuple):\n",
    "        for i in range(len(key) - 1):\n",
    "            reversible_links[(key[i], key[i + 1])] = value[0]  # 使用第一個值作為車道數\n",
    "            reversible_links[(key[i + 1], key[i])] = value[1]  # 使用第二個值作為車道數\n",
    "    else:\n",
    "        reversible_links[key] = value[0]  # 單一連結的情況\n",
    "        reversible_links[(key[1], key[0])] = value[1]  # 反向連結\n",
    "\n",
    "print(\"Reversible links:\", reversible_links)\n",
    "\n",
    "# 遍歷每一行數據\n",
    "for index, row in gdf.iterrows():\n",
    "    # 獲取 A 和 B 的值\n",
    "    A = row['A']\n",
    "    B = row['B']\n",
    "    \n",
    "    # 獲取車道數\n",
    "    lanes = row['LANES_110']\n",
    "    \n",
    "    # 獲取 CLASS_110\n",
    "    class_num = row['CLASS_110']\n",
    "    \n",
    "    # 檢查是否為可調撥車道\n",
    "    if (A, B) in reversible_links:\n",
    "        lanes = reversible_links[(A, B)]\n",
    "        capacity = cap_dict.get((class_num, lanes), 0)\n",
    "    elif (B, A) in reversible_links:\n",
    "        lanes = reversible_links[(B, A)]\n",
    "        capacity = cap_dict.get((class_num, lanes), 0)\n",
    "    else:\n",
    "        capacity = cap_dict.get((class_num, lanes), 0)\n",
    "    \n",
    "    # 更新行數據(LANES_110, capacity)\n",
    "    gdf.at[index, 'LANES_110'] = lanes\n",
    "    gdf.at[index, 'capacity'] = capacity\n",
    "\n",
    "# 保存修改後的 GeoDataFrame\n",
    "gdf.to_file('Y110_modified_v7_updated.shp', driver='ESRI Shapefile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Allen\\AppData\\Local\\Temp\\ipykernel_15752\\1472347535.py:60: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_out = pd.concat([df_out, pd.DataFrame(rows)], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Network\n",
    "\n",
    "input_file = 'Y110_TRTS4S_Net_v7.csv'\n",
    "output_file = 'network.dat'\n",
    "\n",
    "df = pd.read_csv(input_file)\n",
    "# if there is '', then replace with 0\n",
    "df_out = pd.DataFrame(columns=['origin','dest','capacity','length','fft','alpha','beta','speedLimit'])\n",
    "df.fillna(0, inplace=True)\n",
    "rows = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if row[\"LEVEL_110\"] in [1,2,3,4,5,6,7,11]:# and row[\"CLASS_110\"] #not in [i for i in range(84, 99)]+[61]:\n",
    "        # if DIR=1, then A is origin, B is dest\n",
    "        # if DIR=-1, then A is dest, B is origin\n",
    "        # if DIR=2, then the road is bidirectional, so we need to add two rows\n",
    "        # if row[\"DIR\"] == 1 or row[\"DIR\"] == 2:\n",
    "            # print(row[\"ID\"], row[\"DIR\"], row[\"A\"], row[\"B\"])\n",
    "        origin = row[\"A\"]\n",
    "        dest = row[\"B\"]\n",
    "        # elif row[\"DIR\"] == -1:\n",
    "            # print(row[\"ID\"], row[\"DIR\"], row[\"A\"], row[\"B\"])\n",
    "            # origin = row[\"B\"]\n",
    "            # dest = row[\"A\"]\n",
    "        capacity = cap_dict.get((row[\"CLASS_110\"], row[\"LANES_110\"]), 0)\n",
    "        if row[\"CLASS_110\"] == 99:\n",
    "            capacity = 9999\n",
    "        #4359,8218,8251,8252,3677,8260,10923,8270,8271,10928,3646,4018 ID, half capacity\n",
    "        if row[\"ID\"] in [4359, 8218, 8251, 8252, 3677, 8260, 10923, 8270, 8271, 10928, 3646, 4018]:\n",
    "            capacity = capacity/2\n",
    "        #2343 ID, 0.85 capacity\n",
    "        if row[\"ID\"] == 2343:\n",
    "            capacity = capacity*0.85\n",
    "        length = float(row[\"LENGTH\"])  or 1000\n",
    "        try:\n",
    "            fft = length/float(row[\"S0\"])*60\n",
    "        except:\n",
    "            # if ID is not 40438, then give S0=32\n",
    "            # print(row[\"ID\"], row[\"S0\"])\n",
    "            if row[\"ID\"] != 40438:\n",
    "                fft = length/32*60\n",
    "            else:\n",
    "                fft = 1000\n",
    "        alpha = row[\"ALPHA\"] or 0\n",
    "        beta = row[\"BETA\"] or 0\n",
    "        speedLimit = 0\n",
    "        if capacity == 0:\n",
    "            continue\n",
    "        rows.append({'origin':origin,'dest':dest,'capacity':capacity,'length':length,'fft':fft,'alpha':alpha,'beta':beta,'speedLimit':speedLimit})\n",
    "        # if row[\"DIR\"] == 2:\n",
    "            # add the reverse direction\n",
    "            # rows.append({'origin':dest,'dest':origin,'capacity':capacity,'length':length,'fft':fft,'alpha':alpha,'beta':beta,'speedLimit':speedLimit})\n",
    "\n",
    "# write in df_supplement from network_supplement.dat\n",
    "df_supplement = pd.read_csv('network_supplement.dat', sep='\\t', header=None, names=['origin','dest','capacity','length','fft','alpha','beta','speedLimit'])\n",
    "# set columns=['origin','dest','capacity','length','fft','alpha','beta','speedLimit']\n",
    "\n",
    "# tail.node\thead.node\tcapacity..veh.h.\t\tfftt.min.\t\t\tspeed.limit..mph.\n",
    "\n",
    "df_out = pd.concat([df_out, pd.DataFrame(rows)], ignore_index=True)\n",
    "df_out = pd.concat([df_out, df_supplement], ignore_index=True)\n",
    "\n",
    "df_out.to_csv(output_file, index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Allen\\AppData\\Local\\Temp\\ipykernel_23884\\2352710137.py:6: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  df1_dict = df1.set_index(['origin', 'dest']).T.to_dict('list')\n",
      "C:\\Users\\Allen\\AppData\\Local\\Temp\\ipykernel_23884\\2352710137.py:7: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  df_dict = df.set_index(['origin', 'dest']).T.to_dict('list')\n"
     ]
    }
   ],
   "source": [
    "# # read network (1).dat and network.dat\n",
    "# df = pd.read_csv('network.dat', sep='\\t')\n",
    "# df1 = pd.read_csv('network (1).dat', sep='\\t')\n",
    "\n",
    "# #convert df1 and df to dict\n",
    "# df1_dict = df1.set_index(['origin', 'dest']).T.to_dict('list')\n",
    "# df_dict = df.set_index(['origin', 'dest']).T.to_dict('list')\n",
    "\n",
    "# #if origin and dest are in df1, then replace the capacity, length, fft, alpha, beta, speedLimit with df\n",
    "# # if not, then add the row to df1\n",
    "# for key, value in df_dict.items():\n",
    "#     if key in df1_dict:\n",
    "#         # print(key, value, df1_dict[key])\n",
    "#         df1_dict[key][0] = value[0]\n",
    "#         df1_dict[key][1] = value[1]\n",
    "#         df1_dict[key][2] = value[2]\n",
    "#         df1_dict[key][3] = value[3]\n",
    "#         df1_dict[key][4] = value[4]\n",
    "#         df1_dict[key][5] = value[5]\n",
    "#     else:\n",
    "#         # print(key, value)\n",
    "#         df1_dict[key] = value\n",
    "\n",
    "# #convert df1_dict to df1\n",
    "# df1 = pd.DataFrame.from_dict(df1_dict, orient='index', columns=['capacity','length','fft','alpha','beta','speedLimit'])\n",
    "# #index to columns, the tuple is (origin, dest)\n",
    "# df1.reset_index(inplace=True)\n",
    "# df1.rename(columns={'index': 'origin_dest'}, inplace=True)\n",
    "# #split the origin_dest to origin and dest, and it should be the first two columns\n",
    "# df1[['origin', 'dest']] = df1['origin_dest'].apply(pd.Series)\n",
    "# #drop the origin_dest column\n",
    "# df1.drop(columns=['origin_dest'], inplace=True)\n",
    "# #save to network2.dat\n",
    "# df1 = df1[['origin', 'dest', 'capacity', 'length', 'fft', 'alpha', 'beta', 'speedLimit']]\n",
    "# df1.to_csv('network2.dat', index=False, sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Allen\\AppData\\Local\\Temp\\ipykernel_22540\\742789387.py:7: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  df1_dict = df1.set_index(['origin', 'dest']).T.to_dict('list')\n",
      "C:\\Users\\Allen\\AppData\\Local\\Temp\\ipykernel_22540\\742789387.py:8: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  df_dict = df.set_index(['origin', 'dest']).T.to_dict('list')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14043 9070\n",
      "9069 14043\n",
      "1088 10930\n",
      "10930 1088\n",
      "3758\n",
      "3754\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# # read network (1).dat and network.dat\n",
    "# df = pd.read_csv('network.dat', sep='\\t')\n",
    "# df1 = pd.read_csv('network (1).dat', sep='\\t')\n",
    "\n",
    "# #convert df1 and df to dict\n",
    "# df1_dict = df1.set_index(['origin', 'dest']).T.to_dict('list')\n",
    "# df_dict = df.set_index(['origin', 'dest']).T.to_dict('list')\n",
    "\n",
    "# # print the link that df1 has but df doesn't have\n",
    "# count = 0\n",
    "# count_784 = 0\n",
    "# for index, row in df1.iterrows():\n",
    "#     if (row['origin'], row['dest']) not in df_dict:\n",
    "#         count += 1\n",
    "#         # print(row['origin'], row['dest'], row['capacity'], row['length'], row['fft'], row['alpha'], row['beta'], row['speedLimit'])\n",
    "#         # print(int(row['origin']), int(row['dest']))\n",
    "#         if int(row['origin']) <= 784 or int(row['dest']) <= 784:\n",
    "#             count_784 += 1\n",
    "#         else:\n",
    "#             print(int(row['origin']), int(row['dest']))\n",
    "#             # print(row['origin'], row['dest'], row['capacity'], row['length'], row['fft'], row['alpha'], row['beta'], row['speedLimit'])\n",
    "#             # print(int(row['origin']), int(row['dest']))\n",
    "# print(count)\n",
    "# print(count_784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Allen\\AppData\\Local\\Temp\\ipykernel_22540\\1915491137.py:7: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  df1_dict = df1.set_index(['origin', 'dest']).T.to_dict('list')\n",
      "C:\\Users\\Allen\\AppData\\Local\\Temp\\ipykernel_22540\\1915491137.py:8: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  df_dict = df.set_index(['origin', 'dest']).T.to_dict('list')\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# # read network (1).dat and network.dat\n",
    "# df = pd.read_csv('network.dat', sep='\\t')\n",
    "# df1 = pd.read_csv('network (1).dat', sep='\\t')\n",
    "\n",
    "# #convert df1 and df to dict\n",
    "# df1_dict = df1.set_index(['origin', 'dest']).T.to_dict('list')\n",
    "# df_dict = df.set_index(['origin', 'dest']).T.to_dict('list')\n",
    "\n",
    "# # please extract the link that df1 has but df doesn't have\n",
    "# # output as network_supplement.dat\n",
    "# for index, row in df1.iterrows():\n",
    "#     if (row['origin'], row['dest']) not in df_dict:\n",
    "#         # print(row['origin'], row['dest'], row['capacity'], row['length'], row['fft'], row['alpha'], row['beta'], row['speedLimit'])\n",
    "#         # print(int(row['origin']), int(row['dest']))\n",
    "#         with open('network_supplement.dat', 'a') as f:\n",
    "#             f.write(f\"{int(row['origin'])}\\t{int(row['dest'])}\\t{row['capacity']}\\t{row['length']}\\t{row['fft']}\\t{row['alpha']}\\t{row['beta']}\\t{row['speedLimit']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # OD flow\n",
    "# # read TRTS4S_Y110指派OD與交通分區.xlsx\n",
    "# # sheet_name = '110年指派OD晨峰'\n",
    "# # columm A,B,H,I as origin,dest,all,bus\n",
    "# # demand as all+bus*1.8/19\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# input_file = 'TRTS4S_晨昏峰小時指派OD.xlsx'\n",
    "# output_file = 'demand.dat'\n",
    "\n",
    "# df = pd.read_excel(input_file, sheet_name='110年指派OD晨峰小時')\n",
    "# df.fillna(0, inplace=True)\n",
    "# rows = []\n",
    "\n",
    "# print(\"Successfully read the file\")\n",
    "\n",
    "# count=0\n",
    "\n",
    "# for index, row in df.iterrows():\n",
    "#     origin = row[\"I\"]\n",
    "#     dest = row[\"J\"]\n",
    "#     all = row[\"ALL\"]\n",
    "#     bus = row[\"BUS\"]\n",
    "#     demand = all+bus*1.8/19\n",
    "#     if origin <=691 and dest <=691:\n",
    "#         rows.append({'origin':int(origin),'dest':int(dest),'demand':demand})\n",
    "#     if count%10000==0:\n",
    "#         print(count)\n",
    "#     count+=1\n",
    "\n",
    "# df_out = pd.DataFrame(rows)\n",
    "# df_out.to_csv(output_file, index=False, sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 110_晨峰_demand.csv\n",
      "Successfully saved 110_昏峰_demand.csv\n",
      "Successfully saved 140_晨峰_demand.csv\n",
      "Successfully saved 140_昏峰_demand.csv\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# input_file = 'TRTS4S_晨昏峰小時指派OD.xlsx'\n",
    "\n",
    "# for i in [110,140]:\n",
    "#     for j in ['晨峰','昏峰']:\n",
    "#         df = pd.read_excel(input_file, sheet_name=f'{i}年指派OD{j}小時')\n",
    "#         df.fillna(0, inplace=True)\n",
    "\n",
    "#         #save as csv\n",
    "#         output_file = f'{i}_{j}_demand.csv'\n",
    "#         df.to_csv(output_file, index=False, sep=',')\n",
    "#         print(f\"Successfully saved {output_file}\")\n",
    "#\n",
    "#6:55.8 complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 110_晨峰_demand.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "input_file = 'TRTS4S_晨昏峰小時指派OD.xlsx'\n",
    "\n",
    "for i in [110]:\n",
    "    for j in ['晨峰']:\n",
    "        df = pd.read_excel(input_file, sheet_name=f'{i}年指派OD{j}小時')\n",
    "        df.fillna(0, inplace=True)\n",
    "\n",
    "        #save as csv\n",
    "        output_file = f'{i}_{j}_demand.csv'\n",
    "        df.to_csv(output_file, index=False, sep=',')\n",
    "        print(f\"Successfully saved {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 "
     ]
    }
   ],
   "source": [
    "#read 110_晨峰_demand.csv to demand.dat\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('110_晨峰_demand.csv')\n",
    "df.fillna(0, inplace=True)\n",
    "rows = []\n",
    "\n",
    "#482-502\n",
    "tamsui = [i for i in range(482,503)]\n",
    "\n",
    "count=0\n",
    "for index, row in df.iterrows():\n",
    "    origin = row[\"I\"]\n",
    "    dest = row[\"J\"]\n",
    "    all = row[\"ALL\"]\n",
    "    bus = row[\"BUS\"]\n",
    "    demand = all+bus*1.8/19\n",
    "    # if origin within 691 and dest within 691, then demand *= 1.5\n",
    "    # if origin <691 and dest <691:\n",
    "        # demand *= 1.25\n",
    "    # if origin in 304-691 or dest in 304-691, then demand *= 1.5\n",
    "    # if origin <= 691 and dest <= 691 and origin >= 304 and dest >= 304:\n",
    "        # demand *= 1.4\n",
    "    # if dest within 303, then demand *= 1.5\n",
    "    if dest <= 303:\n",
    "        demand *= 1\n",
    "    # if origin in tamsui or dest in tamsui:\n",
    "    if origin in tamsui:\n",
    "        demand *= 1\n",
    "    out_of_cordon=[i for i in range(692,751)]+[783,784]\n",
    "    if origin not in out_of_cordon and dest not in out_of_cordon:\n",
    "        rows.append({'origin':int(origin),'dest':int(dest),'demand':demand})\n",
    "    # if count%10000==0:\n",
    "        # print(count//10000, end=' ')\n",
    "    progress10=[int(i*61465.6) for i in range(1,11)]\n",
    "    if count in progress10:\n",
    "        print(count//61465.6,'%' end=' ')\n",
    "\n",
    "    count+=1\n",
    "\n",
    "df_out = pd.DataFrame(rows)\n",
    "df_out.to_csv('demand.dat', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demand.dat is generating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 614656/614656 [00:36<00:00, 17018.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demand.dat is generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#read 110_晨峰_demand.csv to demand.dat\n",
    "df = pd.read_csv('110_晨峰_demand.csv')\n",
    "df.fillna(0, inplace=True)\n",
    "rows = []\n",
    "\n",
    "#482-502\n",
    "tamsui = [i for i in range(482,503)]\n",
    "\n",
    "count=0\n",
    "\n",
    "#use tqdm to show the progress for 614656 rows\n",
    "print(\"demand.dat is generating...\")\n",
    "for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    origin = row[\"I\"]\n",
    "    dest = row[\"J\"]\n",
    "    all = row[\"ALL\"]\n",
    "    bus = row[\"BUS\"]\n",
    "    demand = all+bus*1.8/19\n",
    "    # if origin within 691 and dest within 691, then demand *= 1.5\n",
    "    # if origin <691 and dest <691:\n",
    "        # demand *= 1.25\n",
    "    # if origin in 304-691 or dest in 304-691, then demand *= 1.5\n",
    "    # if origin <= 691 and dest <= 691 and origin >= 304 and dest >= 304:\n",
    "        # demand *= 1.4\n",
    "    # if dest within 303, then demand *= 1.5\n",
    "    if dest <= 303:\n",
    "        demand *= 1\n",
    "    # if origin in tamsui or dest in tamsui:\n",
    "    if origin in tamsui:\n",
    "        demand *= 1\n",
    "    out_of_cordon=[i for i in range(692,751)]+[783,784]\n",
    "    if origin not in out_of_cordon and dest not in out_of_cordon:\n",
    "        rows.append({'origin':int(origin),'dest':int(dest),'demand':demand})\n",
    "\n",
    "print(\"demand.dat is generated\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
